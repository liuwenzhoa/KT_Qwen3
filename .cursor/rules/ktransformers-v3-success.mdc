---
description: 
globs: 
alwaysApply: true
---
# KTransformers成功运行配置参考

## 概述

KTransformers是一个优化大型语言模型推理性能的框架，可在有限硬件资源下运行DeepSeek-V3等大型模型。本规则整理了已验证可成功运行KTransformers的硬件配置，供用户参考。

## 成功案例配置

### 案例1：服务器级配置

**硬件配置**：
- GPU: NVIDIA L40s (48GB VRAM)
- CPU: 双路Intel Xeon 9654处理器 (共192核心)
- 内存: 768GB DDR5 12通道

**性能指标**：
- 预填充速度: 108 tokens/s
- 解码速度: 10.8 tokens/s

**软件版本**：
- KTransformers: main分支源代码编译版本

### 案例2：高端工作站配置

**硬件配置**：
- GPU: 单张NVIDIA RTX 4090 (24GB VRAM)
- CPU: 双路Intel Xeon 6430 32C处理器 (共64核心128线程)
- 内存: 480GB DDR5

**性能指标**：
- 运行速度: 约6-8 tokens/s

## 其他兼容平台

据用户报告，KTransformers也可在以下平台上运行：
- NVIDIA RTX 2080系列显卡
- AMD GPU (通过ROCm支持)

## 硬件需求分析

### 最低要求

基于成功案例和文档，运行KTransformers(特别是DeepSeek-V3、DeepSeek-R1等大型模型)的最低硬件要求：

1. **GPU**:
   - VRAM: 14GB+(推荐24GB+)
   - 架构: Ampere及以上(RTX 30系列或更新)性能最佳

2. **CPU**:
   - 核心数: 32+(推荐64+)
   - 支持AVX2指令集(必须)，AVX512(推荐)
   - 多处理器系统对MoE模型有显著性能提升

3. **内存**:
   - 容量: 取决于模型大小，DeepSeek-V3建议128GB+
   - 类型: DDR4-3200或更快，DDR5更佳

### 性能与硬件关系

1. **GPU性能影响**:
   - 主要影响注意力计算和预填充速度
   - VRAM大小决定可处理的上下文长度
   - 显存带宽对注意力计算性能至关重要

2. **CPU性能影响**:
   - 核心数直接影响专家并行计算能力
   - 高时钟频率有助于提高单线程性能
   - AVX512支持使专家计算更高效

3. **内存性能影响**:
   - 内存容量决定可加载的模型大小
   - 多通道配置提高内存带宽
   - 对于MoE模型尤为重要

## 报告您的成功配置

为帮助更多用户找到合适的硬件配置，KTransformers团队欢迎用户提交成功运行的环境配置。请通过以下链接提交您的配置信息：

[提交成功配置信息](mdc:https:/docs.qq.com/smartsheet/form/AVxgQOYhhNfl%2FBB08J2%2Fv3rnnq?tab=BB08J2)

## 性能优化建议

1. **GPU优化**:
   - 如有多GPU，考虑使用多GPU配置
   - 使用适当的CUDA Graph设置

2. **CPU优化**:
   - 设置`--cpu_infer`参数为物理核心数略小的值
   - 为获得最佳性能，确保CPU负载不超过90%

3. **内存优化**:
   - 监控内存使用，避免过度分页
   - 根据实际DRAM大小选择合适的上下文长度设置

