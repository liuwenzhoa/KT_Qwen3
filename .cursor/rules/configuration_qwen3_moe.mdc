---
description: 
globs: 
alwaysApply: true
---
# Qwen3Moe 配置指南

[configuration_qwen3_moe.py](mdc:ktransformers/ktransformers/models/configuration_qwen3_moe.py) 文件定义了Qwen3 MoE模型的配置类，包含模型架构的所有超参数设置。

## 配置类概述

`Qwen3MoeConfig` 类继承自huggingface的 `PretrainedConfig`，用于初始化Qwen3 MoE模型的参数。默认配置与 [Qwen/Qwen3-MoE-15B-A2B](mdc:https:/huggingface.co/Qwen/Qwen3-MoE-15B-A2B) 相似。

## 主要配置参数

### 基础参数
- `vocab_size`: 词表大小，默认为151936
- `hidden_size`: 隐藏层维度，默认为2048
- `intermediate_size`: MLP中间层维度，默认为6144
- `num_hidden_layers`: Transformer编码器中的隐藏层数，默认为24
- `num_attention_heads`: 注意力头数，默认为32
- `num_key_value_heads`: 实现GQA的key-value头数，默认为4
- `hidden_act`: 非线性激活函数，默认为"silu"
- `max_position_embeddings`: 支持的最大序列长度，默认为32768
- `initializer_range`: 初始化权重的标准差，默认为0.02
- `rms_norm_eps`: RMS规范化的epsilon值，默认为1e-6

### 注意力相关参数
- `attention_bias`: 是否在自注意力的投影层使用偏置，默认为False
- `attention_dropout`: 注意力概率的dropout率，默认为0.0
- `use_sliding_window`: 是否使用滑动窗口注意力，默认为False
- `sliding_window`: 滑动窗口大小，默认为4096
- `max_window_layers`: 使用滑动窗口注意力的层数，默认为28

### RoPE相关参数
- `rope_theta`: RoPE嵌入的基础周期，默认为10000.0
- `rope_scaling`: RoPE缩放配置，支持多种扩展方式

### MoE相关参数
- `decoder_sparse_step`: MoE层的频率，默认为1
- `moe_intermediate_size`: 路由专家的中间层大小，默认为768
- `num_experts_per_tok`: 每个token选择的专家数，默认为8
- `num_experts`: 路由专家的数量，默认为128
- `norm_topk_prob`: 是否规范化topk概率，默认为False
- `output_router_logits`: 是否返回路由器logits，默认为False
- `router_aux_loss_coef`: 路由辅助损失系数，默认为0.001
- `mlp_only_layers`: 指定使用标准MLP而非MoE的层索引，默认为[]

## 张量并行计划

文件还定义了模型的张量并行和流水线并行计划：

```python
base_model_tp_plan = {
    "layers.*.self_attn.q_proj": "colwise",
    "layers.*.self_attn.k_proj": "colwise",
    "layers.*.self_attn.v_proj": "colwise",
    "layers.*.self_attn.o_proj": "rowwise",
    "layers.*.mlp.gate_proj": "colwise",
    "layers.*.mlp.up_proj": "colwise",
    "layers.*.mlp.down_proj": "rowwise",
}
```

## 使用示例

```python
from transformers import Qwen3MoeModel, Qwen3MoeConfig

# 初始化Qwen3MoeConfig配置
config = Qwen3MoeConfig(
    num_hidden_layers=32,
    hidden_size=4096,
    num_attention_heads=32,
    num_experts=16,
    num_experts_per_tok=2
)

# 使用配置初始化模型
model = Qwen3MoeModel(config)
```

## 注意事项

- 启用`output_router_logits`时，模型会输出路由器logits，并允许计算辅助损失
- MoE结构的性能和效率很大程度上取决于专家数量和每个token选择的专家数
- 在大规模分布式训练中，可以利用`base_model_tp_plan`和`base_model_pp_plan`实现高效的模型并行

