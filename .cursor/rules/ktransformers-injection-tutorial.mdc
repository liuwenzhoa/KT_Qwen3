---
description: 
globs: 
alwaysApply: true
---
# KTransformers 算子注入教程

## 概述

KTransformers是一个灵活的Python框架，用于通过高级内核优化和放置/并行策略增强Transformers的性能。其核心功能之一是算子注入系统，允许用户将原始PyTorch模块替换为优化的变体。本规则详细介绍这一机制。

## 注入规则基本结构

注入规则使用YAML格式定义，基本结构如下：

```yaml
- match:
    name: "^model\\.layers\\..*\\.*$"  # 目标模块名称(正则表达式)
    class: torch.nn.Linear  # 目标模块类型
  replace:
    class: "default" # 替换的算子类名(default表示不替换类，仅更改参数)
    kwargs:
      generate_device: "cuda:0" # 指定运行设备
      # 其他自定义参数
  recursive: True # 是否递归注入子模块
```

**关键字段说明**：
- `match`: 定义匹配条件，支持名称(正则表达式)和类型两种匹配方式
- `replace`: 指定替换的模块类和初始化参数
- `recursive`: 控制是否递归地应用到子模块

## 理解模型结构

为了正确编写注入规则，需要了解模型的结构。以DeepSeek-V2为例：

1. 从模型的`.safetensors`文件可获取权重名称(对应`match.name`)
2. 从`modeling_deepseek.py`可了解模块的类实现(对应`match.class`)

DeepSeek-V2模型的整体结构包括：
- 输入嵌入层
- 多层Transformer块
  - MLA注意力机制
  - MoE(Mixture of Experts)前馈网络
- 输出层

## 支持的算子类型

KTransformers支持多种优化算子替换：

| 原始模块类型 | 替换算子               | 后端选项                | 说明                   |
|------------|----------------------|------------------------|------------------------|
| Linear     | KTransformersLinear  | KLinearMarlin          | 使用Marlin作为后端      |
|            |                      | KLinearTorch           | 使用PyTorch作为后端     |
|            |                      | KLinearCPUInfer        | 使用Llamafile作为后端   |
|            |                      | KLinearFP8             | Triton fp8_gemm内核    |
| Experts    | KTransformersExperts | KExpertsTorch          | 使用PyTorch作为后端     |
|            |                      | KExpertsMarlin         | 使用Marlin作为后端      |
|            |                      | KExpertsCPU            | 使用Llamafile作为后端   |
| Attention  | KDeepseekV2Attention | -                      | 优化的MLA实现           |
| MoE        | KMistralSparseMoEBlock | -                    | Qwen2的MoE实现         |
|            | KDeepseekV2MoE       | -                      | DeepseekV2的MoE实现    |
| RoPE       | RotaryEmbedding      | -                      | 位置编码实现            |
|            | YarnRotaryEmbedding  | -                      | 增强的位置编码实现       |

## 注入示例

### 注入MLA注意力机制

```yaml
- match:
    name: "^model\\.layers\\..*\\.self_attn$"
  replace:
    class: ktransformers.operators.attention.KDeepseekV2Attention
    kwargs:
      generate_device: "cuda"
      prefill_device: "cuda"
```

### 注入专家模块(MoE)

```yaml
- match:
    name: "^model\\.layers\\..*\\.mlp\\.experts$"
  replace:
    class: ktransformers.operators.experts.KTransformersExperts
    kwargs:
      generate_device: "cpu"
      generate_op: "MLPCPUExperts"
      out_device: "cuda"
  recursive: False
```

### 注入线性层

```yaml
- match:
    name: "^model\\.layers\\.(?!.*self_attn).*$"
    class: torch.nn.Linear
  replace:
    class: ktransformers.operators.linear.KTransformersLinear
    kwargs:
      generate_device: "cuda"
      generate_op: "QuantizedLinearMarlin"
```

## 自定义算子开发

开发自定义算子需要继承`BaseInjectedModule`类，基本结构如下：

```python
class CustomLinearInject(BaseInjectedModule):
    def __init__(
        self,
        key: str,
        gguf_loader: GGUFLoader,
        config: PretrainedConfig,
        orig_module: nn.Module = None,
        generate_device: str = "cuda",
        my_param: bool = True,  # 自定义参数
        **kwargs,
    ):
        super().__init__(key, gguf_loader, config, orig_module, generate_device, **kwargs)
        self.my_param = my_param
        
    # 实现必要的方法(load, unload, forward)
```

对于线性层，可继承`KLinearBase`获取额外辅助功能。

## 注入过程

注入过程的基本流程是：
1. 使用meta设备初始化原始模型(避免内存占用)
2. 调用`optimize_and_load_gguf`加载权重并注入算子
3. 使用模型的标准接口进行推理


完整的注入配置文件示例可在 [DeepSeek-V2-Chat.yaml](mdc:ktransformers/ktransformers/optimize/optimize_rules/DeepSeek-V2-Chat.yaml) 中找到。